{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a predetermined seed so all our results can be replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('Dataset/clean_data.csv', index_col='Customer_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate target column with rest of the data\n",
    "churn_col = full_data['churn'].copy()\n",
    "full_data = full_data.drop('churn',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're finished with EDA in the other notebook, we can start splitting the data into training and testing sets. For models that need validation, we will utilize k-fold CV later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data for training and testing with 80% for training and 20% testing\n",
    "#Uses our preselected random seed to results are reproducible \n",
    "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
    "    full_data,\n",
    "    churn_col,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columnDescriptions.json','r') as f:\n",
    "    col_desc = json.load(f)\n",
    "    \n",
    "#Shortened descriptions with elipses for plot titles\n",
    "#Only retains first 20 characters of description then appends with elipses\n",
    "short_col_desc = dict(zip(\n",
    "    col_desc.keys(),\n",
    "    map(lambda desc: \n",
    "        desc if len(desc)<20 else f'{desc[:20]}...', col_desc.values()\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use several different algorithms then compare their performance afterwards to determine which is the best to use. The algorithm we will use are: \n",
    "- Logistic Regression\n",
    "- K Nearest Neighbor Classifier\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "Since the implementations selected for the above algorithms have differing aptitudes for missing values and normalization, we will need different preprocessing pipelines of the data. For example, XGBoost and LightGBM can handle nan values (XGBoost learns whether to split nan values during training, while LightGBM allocates nan values to reduce loss afterwards) while the sklearn implementations of Logisitic regression, KNN classifier, and random forest cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import PipelineFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PipelineFactory(full_data)\n",
    "pca_pipe = pf.create_pipe(pca=True,impute=True,normalize=True)\n",
    "# impute_normalize_pipe = pf.create_pipe(impute=True,normalize=True)\n",
    "# impute_pipe = pf.create_pipe(impute=True,normalize=False)\n",
    "# ohe_pipe = pf.create_pipe(impute=False,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pca0', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7',\n",
       "       'pca8', 'pca9', 'pca10', 'pca11', 'pca12', 'pca13', 'pca14',\n",
       "       'pca15', 'pca16', 'pca17', 'pca18', 'pca19', 'pca20', 'pca21',\n",
       "       'pca22', 'pca23', 'pca24', 'pca25', 'pca26', 'pca27', 'pca28',\n",
       "       'pca29', 'pca30', 'pca31', 'pca32', 'pca33', 'pca34', 'pca35',\n",
       "       'pca36', 'pca37', 'pca38', 'pca39', 'pca40', 'pca41', 'pca42',\n",
       "       'pca43', 'pca44', 'pca45', 'pca46', 'pca47', 'pca48', 'pca49',\n",
       "       'pca50', 'pca51', 'pca52', 'pca53', 'pca54', 'pca55', 'pca56',\n",
       "       'pca57', 'pca58', 'pca59', 'pca60', 'pca61', 'pca62', 'pca63',\n",
       "       'pca64', 'pca65', 'pca66', 'pca67', 'pca68', 'pca69', 'pca70',\n",
       "       'pca71', 'pca72', 'pca73'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_pipe.transformer_list[0][1].steps[-1][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pca_pipe.fit_transform(raw_x_train)\n",
    "x_test = impute_normalize_pipe.transform(raw_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "# 5 default chosen regularization strength\n",
    "# 5 fold CV (80% training 20% validation)\n",
    "log_reg = LogisticRegressionCV(\n",
    "    Cs=\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight='balanced'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('telecomChurn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbdce887e3e5f960b74eaae67cd2ebc9c84860334727f842ebfba4f686e38cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
